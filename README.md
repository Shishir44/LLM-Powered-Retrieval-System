# LLM-Powered Retrieval System

A production-ready, microservices-based retrieval system powered by advanced RAG capabilities, built with FastAPI, LangChain, and modern containerization.

## ğŸš€ Quick Start (2 minutes)

```bash
# 1. Clone and setup
git clone <your-repo>
cd LLM-Powered-Retrieval-System

# 2. Configure environment
cp .env.example .env
# Edit .env with your API keys (OPENAI_API_KEY, PINECONE_API_KEY)

# 3. Start everything
./quick-start.sh

# 4. Test the system
./test_complete_workflow.sh
```

## ğŸ—ï¸ Architecture Overview

### Microservices Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Docker Compose                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Knowledge Base  â”‚ Conversation    â”‚ Analytics           â”‚
â”‚ Service :8002   â”‚ Service :8001   â”‚ Service :8005       â”‚
â”‚                 â”‚                 â”‚                     â”‚
â”‚ â€¢ Document CRUD â”‚ â€¢ Chat API      â”‚ â€¢ Quality Metrics  â”‚
â”‚ â€¢ Vector Search â”‚ â€¢ RAG Pipeline  â”‚ â€¢ User Feedback    â”‚
â”‚ â€¢ Hybrid        â”‚ â€¢ Streaming     â”‚ â€¢ Prometheus       â”‚
â”‚   Retrieval     â”‚ â€¢ Context Mgmt  â”‚   Metrics          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Infrastructure Services                        â”‚
â”‚ PostgreSQL | Redis | Prometheus | Grafana              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Independent Services
- **âœ… Complete isolation** - Each service has its own dependencies, Docker container, and lifecycle
- **âœ… Independent deployment** - Services can be built, tested, and deployed separately  
- **âœ… Fault tolerance** - Failure of one service doesn't affect others
- **âœ… Horizontal scaling** - Scale services independently based on load

## ğŸ“ Project Structure

```
LLM-Powered-Retrieval-System/
â”œâ”€â”€ ğŸ”§ Setup & Configuration
â”‚   â”œâ”€â”€ .env.example                    # Environment template
â”‚   â”œâ”€â”€ docker-compose.yml              # Multi-service orchestration
â”‚   â”œâ”€â”€ setup.sh                        # Full setup script
â”‚   â””â”€â”€ quick-start.sh                  # 2-minute quick start
â”‚
â”œâ”€â”€ ğŸ§ª Testing & Quality
â”‚   â”œâ”€â”€ test_complete_workflow.sh       # End-to-end testing
â”‚   â”œâ”€â”€ TESTING_GUIDE.md               # Comprehensive test guide
â”‚   â”œâ”€â”€ postman_collection.json        # API test collection
â”‚   â””â”€â”€ load_tests/                     # Performance testing
â”‚
â”œâ”€â”€ ğŸš€ Services (Independent Microservices)
â”‚   â”œâ”€â”€ knowledge-base-service/         # Document & RAG operations
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI app
â”‚   â”‚   â”‚   â”œâ”€â”€ core/                  # Business logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py       # Advanced RAG retriever
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py        # Document processing
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ cache.py           # Vector caching
â”‚   â”‚   â”‚   â””â”€â”€ api/routes.py          # REST endpoints
â”‚   â”‚   â”œâ”€â”€ Dockerfile                 # Container definition
â”‚   â”‚   â”œâ”€â”€ requirements.txt           # Service dependencies
â”‚   â”‚   â””â”€â”€ README.md                  # Service documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ conversation-service/           # Chat & conversation management
â”‚   â”‚   â”œâ”€â”€ src/core/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py        # Multi-stage RAG
â”‚   â”‚   â”‚   â”œâ”€â”€ context_manager.py     # Conversation context
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming.py           # Real-time responses
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py             # LLM prompt templates
â”‚   â”‚   â””â”€â”€ [same structure as above]
â”‚   â”‚
â”‚   â””â”€â”€ analytics-service/              # Metrics & evaluation
â”‚       â”œâ”€â”€ src/core/rag_metrics.py    # Quality evaluation
â”‚       â””â”€â”€ [same structure as above]
â”‚
â””â”€â”€ ğŸ—ï¸ Infrastructure
    â””â”€â”€ customer-support-platform/infrastructure/
        â”œâ”€â”€ kubernetes/                 # K8s deployment manifests
        â””â”€â”€ monitoring/                 # Prometheus configuration
```

## ğŸ”¥ Key Features

### ğŸ¤– Advanced RAG Capabilities
- **Hybrid Retrieval**: Vector similarity + BM25 + Contextual compression
- **Multi-stage Pipeline**: Query rewriting â†’ Multi-query retrieval â†’ Reranking
- **Context-aware Responses**: Conversation history + User intent + Sentiment analysis
- **Streaming Responses**: Real-time response generation

### ğŸ›ï¸ Production-Ready Architecture
- **Independent Services**: True microservices with separate containers
- **Health Monitoring**: Comprehensive health checks and metrics
- **Horizontal Scaling**: Kubernetes-ready with HPA support
- **Fault Tolerance**: Circuit breakers and graceful degradation

### ğŸ“Š Quality & Observability
- **RAG Quality Metrics**: Retrieval precision, response relevance, context utilization
- **User Feedback Loop**: Satisfaction scoring and continuous improvement
- **Comprehensive Monitoring**: Prometheus + Grafana dashboards
- **Distributed Tracing**: Request tracing across services

## ğŸ› ï¸ Development

### Local Development
```bash
# Start individual service for development
cd knowledge-base-service
pip install -r requirements.txt  
python -m src.main

# Or use Docker for consistency
docker-compose up knowledge-base-service
```

### Testing
```bash
# Quick health check
curl http://localhost:8001/health

# Run full test suite
./test_complete_workflow.sh

# Load testing
./load_tests/run_load_tests.sh

# Import Postman collection
# File: postman_collection.json
```

### API Documentation
- **Knowledge Base**: http://localhost:8002/docs
- **Conversation**: http://localhost:8001/docs  
- **Analytics**: http://localhost:8005/docs

## ğŸš¢ Deployment

### Docker Compose (Development/Staging)
```bash
docker-compose up -d
```

### Kubernetes (Production)
```bash
kubectl apply -f customer-support-platform/infrastructure/kubernetes/
```

### Environment Variables
Key configurations in `.env`:
- `OPENAI_API_KEY` - OpenAI API access
- `PINECONE_API_KEY` - Vector database access
- `VECTOR_STORE_TYPE` - Vector database type (pinecone/weaviate/chroma)
- Service URLs for inter-service communication

## ğŸ” Monitoring & Observability

### Dashboards
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **Service Metrics**: Each service exposes `/metrics` endpoint

### Key Metrics
- **Response Quality**: Retrieval precision, response relevance
- **Performance**: Response times, throughput, error rates
- **System Health**: Service uptime, resource utilization
- **User Satisfaction**: Feedback scores, conversation success rates

## ğŸ¤ API Usage Examples

### Create and Search Documents
```bash
# Create document
curl -X POST "http://localhost:8002/api/v1/documents" \
  -H "Content-Type: application/json" \
  -d '{"title":"API Guide","content":"How to use our API...","category":"docs"}'

# Search documents  
curl "http://localhost:8002/api/v1/search?q=API%20guide&limit=5"
```

### Chat Conversation
```bash
# Send chat message
curl -X POST "http://localhost:8001/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{"message":"How do I use the API?","conversation_id":"chat-1"}'
```

### Quality Evaluation
```bash
# Evaluate response quality
curl -X POST "http://localhost:8005/api/v1/evaluate" \
  -H "Content-Type: application/json" \
  -d '{"query":"...","context":"...","response":"..."}'
```

## ğŸ“‹ System Requirements

- **Docker** & **Docker Compose**
- **API Keys**: OpenAI, Vector Database (Pinecone/Weaviate)
- **Ports**: 8001, 8002, 8005, 9090, 3000
- **Memory**: 4GB+ recommended
- **Storage**: Vector database + PostgreSQL

## ğŸ†˜ Troubleshooting

### Common Issues
- **Services not starting**: Check API keys in `.env`
- **Port conflicts**: Ensure ports 8001, 8002, 8005 are available
- **Connection errors**: Verify Docker network and service communication
- **API errors**: Check service logs with `docker-compose logs [service-name]`

### Get Help
```bash
# View service logs
docker-compose logs knowledge-base-service

# Check service health
curl http://localhost:8002/health

# Run diagnostics
./test_complete_workflow.sh
```

## ğŸ¯ Success Criteria

Your system is working correctly when:
- âœ… All health endpoints return `200 OK`
- âœ… Documents can be created and searched
- âœ… Conversations generate coherent responses
- âœ… Analytics track quality metrics
- âœ… Test script passes all checks

## ğŸ“ˆ What's Next?

- **Scale**: Deploy to Kubernetes for production
- **Extend**: Add more vector stores or LLM providers
- **Optimize**: Fine-tune retrieval and ranking algorithms
- **Integrate**: Connect with external APIs and data sources
- **Monitor**: Set up alerts and performance optimization

---

**ğŸ‰ Ready to build amazing RAG applications!**

For detailed testing instructions, see [TESTING_GUIDE.md](TESTING_GUIDE.md)